{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5092fdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import keras\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f97af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the shapefile or pickle which we created in last article\n",
    "df=gpd.read_file(\"points_data.shp\")\n",
    "# df=pd.read_pickle(\"points_data.pkl\") # in case of pickle\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a96729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that there is no no data values in the dataset\n",
    "print(df.isnull().sum())\n",
    "#df = df.dropna() # use this to remove rows with no data values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b7430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understand the data \n",
    "#Here we can see that we have a balanced dataset (equal number of flooded and non flooeded locations\n",
    "sns.countplot(x=\"Label\", data=df) #0 - Notflooded   1 - Flooded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4924dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the correlation matric for the dataset\n",
    "corrMatrix = df.corr()\n",
    "fig, ax = plt.subplots(figsize=(10,10))         # Sample figsize in inches\n",
    "#sns.heatmap(df.iloc[:, 1:6:], annot=True, linewidths=.5, ax=ax)\n",
    "sns.heatmap(corrMatrix, annot=True, linewidths=.5, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd74fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the dependent variable that needs to be predicted (labels)\n",
    "Y = df[\"Label\"].values\n",
    "\n",
    "#Define the independent variables. Let's also drop gemotry and label\n",
    "X = df.drop(labels = [\"Label\", \"geometry\"], axis=1) \n",
    "features_list = list(X.columns)  #List features so we can rank their importance later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f182a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to convert X from dataframe to array to train the neural netowrk\n",
    "X_arr=X.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c85cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into train (60 %), validate (20 %) and test (20%) to verify accuracy after fitting the model.\n",
    "# training data is used to train the model\n",
    "# validation data is used for hyperparameter tuning\n",
    "# testing data is used to test the model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, Y, test_size=0.2,shuffle=True, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25,shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd8ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import Dropout\n",
    "\n",
    "NAME= \"DNN\"\n",
    "# here is the network layers\n",
    "model =Sequential([\n",
    "    Dense(9, activation='relu', input_shape=(11,)),\n",
    "    Dropout(0.1),\n",
    "    Dense(64, activation= 'relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(64, activation= 'relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(64, activation= 'relu'),\n",
    "    Dense(1,activation= 'sigmoid'),\n",
    "])\n",
    "\n",
    "\n",
    "# save the model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "checkpoint = ModelCheckpoint(\"DNN.h5\", monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "# Early stopping to stop the network when there is no improvement based on the validation loss\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "\n",
    "# you can see the training and validation losses for each epoch \n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1fa477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "history=model.fit(X_train,y_train,epochs=500,verbose=2,batch_size=64,validation_split=0.25, callbacks=[checkpoint,early,tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a6be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the training and validation loss at each epoch\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0086654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the training and validation accuracy at each epoch\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "plt.plot(epochs, acc, 'y', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4da8825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "_, acc = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy = \", (acc * 100.0), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b44405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the confusion matrix for various thresholds. Which one is good?\n",
    "#Need to balance positive, negative, false positive and false negative. \n",
    "#ROC can help identify the right threshold.\n",
    "#Receiver Operating Characteristic (ROC) Curve is a plot that helps us \n",
    "#visualize the performance of a binary classifier when the threshold is varied. \n",
    "#ROC\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "y_preds = model.predict(X_test).ravel()\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_preds)\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'y--')\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdb7252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC\n",
    "#Area under the curve (AUC) for ROC plot can be used to understand how well a classifier is performing. \n",
    "#% chance that the model can distinguish between positive and negative classes.\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "auc_value = auc(fpr, tpr)\n",
    "print(\"Area under curve, AUC = \", auc_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2973467",
   "metadata": {},
   "source": [
    "## Map the whole study area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c3d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read shapefile for the whole study area\n",
    "df_SA=gpd.read_file(\"Study_area.shp\")\n",
    "df_SA.head() # make sure that the dataset has the same column arrangement as the training dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc02a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_SA= df_SA.drop(labels = [\"geometry\"], axis=1) # we need to remove all the columns except the predictive features\n",
    "X_SA.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b38396",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_SA = model.predict(X_SA.to_numpy()) # predict if the location is flooded (1) or not flooded (0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34870e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to map the flood susceptibility we need to cacluate the probability of being flooded\n",
    "prediction_prob=model.predict_proba(X_SA) # This function return an array with lists \n",
    "# each list has two values [probability of being not flooded , probability of being flooded]\n",
    "\n",
    "# We need only the probablity of being flooded\n",
    "# We need to add the value coressponding to each point\n",
    "\n",
    "df_SA['FSM']= prediction_prob[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47144741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe tp a shapefile in case of converting the points to raster using QGIS or Arcmap\n",
    "df_SA.to_file(\"FSM.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c83187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the point shapefile to raster.\n",
    "# We will use the model prediction (column FSM in df_SA to make a raster)\n",
    "from geocube.api.core import make_geocube\n",
    "import rasterio as rio\n",
    "\n",
    "out_grid= make_geocube(vector_data=df_SA, measurements=[\"FSM\"], resolution=(-1, 1)) #for most crs negative comes first in resolution\n",
    "out_grid[\"FSM\"].rio.to_raster(\"Flood_susceptibility.tif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
