{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5092fdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f97af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the shapefile or pickle which we created in last article\n",
    "df=gpd.read_file(\"points_data.shp\")\n",
    "# df=pd.read_pickle(\"points_data.pkl\") # in case of pickle\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a96729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that there is no no data values in the dataset\n",
    "print(df.isnull().sum())\n",
    "#df = df.dropna() # use this to remove rows with no data values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b7430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understand the data \n",
    "#Here we can see that we have a balanced dataset (equal number of flooded and non flooeded locations\n",
    "sns.countplot(x=\"Label\", data=df) #0 - Notflooded   1 - Flooded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4924dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the correlation matric for the dataset\n",
    "corrMatrix = df.corr()\n",
    "fig, ax = plt.subplots(figsize=(10,10))         # Sample figsize in inches\n",
    "#sns.heatmap(df.iloc[:, 1:6:], annot=True, linewidths=.5, ax=ax)\n",
    "sns.heatmap(corrMatrix, annot=True, linewidths=.5, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd74fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the dependent variable that needs to be predicted (labels)\n",
    "Y = df[\"Label\"].values\n",
    "\n",
    "#Define the independent variables. Let's also drop gemotry and label\n",
    "X = df.drop(labels = [\"Label\", \"geometry\"], axis=1) \n",
    "features_list = list(X.columns)  #List features so we can rank their importance later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c85cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into train (60 %), validate (20 %) and test (20%) to verify accuracy after fitting the model.\n",
    "# training data is used to train the model\n",
    "# validation data is used for hyperparameter tuning\n",
    "# testing data is used to test the model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, Y, test_size=0.2,shuffle=True, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25,shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd8ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='rbf',probability=True) # I am using the default values of the parameters.\n",
    "\n",
    "# Train the model on training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e64cc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction for the test dataset.\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "# The prediction values are either 1 (Flooded) or 0 (Non-Flooded) \n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db717a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The AUC is considered one of the best performance indices\n",
    "# We can plot the curve and calculate it\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "ax = plt.gca()\n",
    "model_disp = plot_roc_curve(model, X_test, y_test, ax=ax, alpha=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2973467",
   "metadata": {},
   "source": [
    "## Map the whole study area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c3d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read shapefile for the whole study area\n",
    "df_SA=gpd.read_file(\"Study_area.shp\")\n",
    "df_SA.head() # make sure that the dataset has the same column arrangement as the training dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc02a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_SA= df_SA.drop(labels = [\"geometry\"], axis=1) # we need to remove all the columns except the predictive features\n",
    "X_SA.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b38396",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_SA = model.predict(X_SA) # predict if the location is flooded (1) or not flooded (0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34870e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to map the flood susceptibility we need to cacluate the probability of being flooded\n",
    "prediction_prob=model.predict_proba(X_SA) # This function return an array with lists \n",
    "# each list has two values [probability of being not flooded , probability of being flooded]\n",
    "\n",
    "# We need only the probablity of being flooded\n",
    "# We need to add the value coressponding to each point\n",
    "\n",
    "df_SA['FSM']= prediction_prob[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47144741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe tp a shapefile in case of converting the points to raster using QGIS or Arcmap\n",
    "df_SA.to_file(\"FSM.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c83187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the point shapefile to raster.\n",
    "# We will use the model prediction (column FSM in df_SA to make a raster)\n",
    "from geocube.api.core import make_geocube\n",
    "import rasterio as rio\n",
    "\n",
    "out_grid= make_geocube(vector_data=df_SA, measurements=[\"FSM\"], resolution=(-1, 1)) #for most crs negative comes first in resolution\n",
    "out_grid[\"FSM\"].rio.to_raster(\"Flood_susceptibility.tif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
