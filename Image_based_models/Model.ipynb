{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "import os\n",
    "import cv2\n",
    "#import keras \n",
    "#from keras.utils import normalize\n",
    "#from keras import *\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data and prepare it for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the preditive features and the label (Notflooded=0 , flooded=1)\n",
    "data_path=r\"D:\\Predictive_features\" # created in data_preperation script\n",
    "CATEGORIES= [\"NotFlooded\", \"Flooded\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the images\n",
    "# plot the first band in the first image \n",
    "for category in CATEGORIES:\n",
    "    path=os.path.join(data_path,category) # Path to flooded and NotFlooded dir\n",
    "    for img in os.listdir(path):\n",
    "        img_open=rasterio.open(os.path.join(path,img))\n",
    "        img_array=img_open.read(1) # open the image and read the band\n",
    "        img_array= np.where(img_array < 0, np.nan,img_array)\n",
    "        mean=np.nanmean(img_array)\n",
    "        img_array= np.where(np.isnan(img_array), mean,img_array)\n",
    "        plt.imshow(img_array,cmap=\"gray\")\n",
    "        plt.show()\n",
    "        #print(img_array)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,12):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the DEM and save it in a list\n",
    "\n",
    "DEM=[]\n",
    "Slope=[]\n",
    "TWI=[]\n",
    "DTRoad=[]\n",
    "DTRiver=[]\n",
    "CN=[]\n",
    "Rain=[]\n",
    "Aspect=[]\n",
    "Curve=[]\n",
    "Freq=[]\n",
    "DTDrainage=[]\n",
    "y=[]\n",
    "# we have 11 predictive features \n",
    "# every feature is presented in one band \n",
    "# loop over the predictive features\n",
    "predictive_features=[DEM, Slope, TWI, DTRoad, DTRiver, CN, Rain, Aspect, Curve, Freq, DTDrainage] # the features muss be in the same order as the bands in the composite raster\n",
    "\n",
    "print(len(predictive_features))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data():\n",
    "    for i in range(len(predictive_features)):\n",
    "        print(i+1)\n",
    "        for category in CATEGORIES:\n",
    "            path= os.path.join(data_path,category) # Path to flooded and NotFlooded dir\n",
    "            class_num= CATEGORIES.index(category)\n",
    "            for img in os.listdir(path):\n",
    "                try:\n",
    "                    img_open=rasterio.open(os.path.join(path,img))\n",
    "                    print(category,img,class_num)\n",
    "                    img_array=img_open.read(i+1)\n",
    "                    #img_array= np.where(img_array < 0, np.nan,img_array)\n",
    "                    #mean=np.nanmean(img_array)\n",
    "                    #img_array= np.where(np.isnan(img_array), mean,img_array)\n",
    "                    #new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
    "                    predictive_features[i].append(img_array)\n",
    "                    \n",
    "                    if i==0:\n",
    "                        y.append(class_num)\n",
    "                        #print(class_num)\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(DEM[0]) # check image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the first image to check that the function is working\n",
    "plt.imshow(DEM[0],cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of images \n",
    "len(DEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the predictive feature lists to numpy arrays\n",
    "DEM_array=np.array(DEM).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "Slope_array=np.array(Slope).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "TWI_array=np.array(TWI).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "DTRoad_array=np.array(DTRoad).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "DTRiver_array=np.array(DTRiver).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "CN_array=np.array(CN).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "DTDrainage_array=np.array(DTDrainage).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "Aspect_array=np.array(Aspect).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "Curvature_array=np.array(Curve).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "Freq_Curve_array=np.array(Freq).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "Rain_array=np.array(Rain).reshape(-1, IMG_SIZE, IMG_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEM_array.shape\n",
    "# number of images x image size x image size x number of bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the predicvtive feature arrays into one array\n",
    "X_array=np.concatenate([DEM_array,Slope_array,TWI_array,DTRoad_array, DTRiver_array,CN_array,Rain_array,Aspect_array, Curvature_array, Freq_Curve_array, DTDrainage_array], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide to train, validate and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training (60%), validation (20%) and testing (20%)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(X_array,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "NAME = \"LeNet\"\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "#Layer 1\n",
    "#Conv Layer 1\n",
    "model.add(Conv2D(filters = 6, \n",
    "                 kernel_size = 5, \n",
    "                 strides = 1, \n",
    "                 activation = 'relu', \n",
    "                 input_shape = (23,23,11)))\n",
    "#Pooling layer 1\n",
    "model.add(MaxPooling2D(pool_size = 2, strides = 2))\n",
    "\n",
    "#add a droupout\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "#Layer 2\n",
    "#Conv Layer 2\n",
    "model.add(Conv2D(filters = 16, \n",
    "                 kernel_size = 5,\n",
    "                 strides = 1,\n",
    "                 activation = 'relu',\n",
    "                 input_shape = (14,14,6)))\n",
    "#Pooling Layer 2\n",
    "model.add(MaxPooling2D(pool_size = 2, strides = 2))\n",
    "\n",
    "#add a droupout\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "#Flatten\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "#Layer 3\n",
    "#Fully connected layer 1\n",
    "model.add(Dense(units = 120, activation = 'relu'))\n",
    "\n",
    "#add a droupout\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "#Layer 4\n",
    "#Fully connected layer 2\n",
    "model.add(Dense(units = 84, activation = 'relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "\n",
    "#Layer 5\n",
    "#Output Layer\n",
    "model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "checkpoint = ModelCheckpoint(\"LeNet.h5\", monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(x_train ,y_train, batch_size=1024, epochs = 1000, validation_split=0.25, callbacks=[checkpoint,early,tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "plt.plot(epochs, acc, 'y', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, acc = model.evaluate(x_test, y_test)\n",
    "print(\"Accuracy = \", (acc * 100.0), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "#We compare labels and plot them based on correct or wrong predictions.\n",
    "#Since sigmoid outputs probabilities we need to apply threshold to convert to label.\n",
    "\n",
    "mythreshold=0.5\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = (model.predict(x_test)>= mythreshold).astype(int)\n",
    "cm=confusion_matrix(y_test, y_pred)  \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['NotFlooded', 'Flooded']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "cohen_kappa_score(y_test, y_pred, labels=None, weights=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the confusion matrix for various thresholds. Which one is good?\n",
    "#Need to balance positive, negative, false positive and false negative. \n",
    "#ROC can help identify the right threshold.\n",
    "#Receiver Operating Characteristic (ROC) Curve is a plot that helps us \n",
    "#visualize the performance of a binary classifier when the threshold is varied. \n",
    "#ROC\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "y_preds = model.predict(x_test).ravel()\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_preds)\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'y--')\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One way to find the best threshold once we calculate the true positive \n",
    "#and false positive rates is ...\n",
    "#The optimal cut off point would be where “true positive rate” is high \n",
    "#and the “false positive rate” is low. \n",
    "#Based on this logic let us find the threshold where tpr-(1-fpr) is zero (or close to 0)\n",
    "\n",
    "import pandas as pd\n",
    "i = np.arange(len(tpr)) \n",
    "roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'thresholds' : pd.Series(thresholds, index=i)})\n",
    "ideal_roc_thresh = roc.iloc[(roc.tf-0).abs().argsort()[:1]]  #Locate the point where the value is close to 0\n",
    "print(\"Ideal threshold is: \", ideal_roc_thresh['thresholds']) \n",
    "\n",
    "#Now use this threshold value in the confusion matrix to visualize the balance\n",
    "#between tp, fp, fp, and fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC\n",
    "#Area under the curve (AUC) for ROC plot can be used to understand how well a classifier is performing. \n",
    "#% chance that the model can distinguish between positive and negative classes.\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "auc_value = auc(fpr, tpr)\n",
    "print(\"Area under curve, AUC = \", auc_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
